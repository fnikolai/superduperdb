"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[1346],{2053:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var i=r(4848),t=r(8453);const a={},o="Transformers",s={id:"docs/ai_integrations/transformers",title:"Transformers",description:"Transformers is a popular AI framework, and we have incorporated native support for Transformers to provide essential Large Language Model (LLM) capabilities.",source:"@site/content/docs/ai_integrations/transformers.md",sourceDirName:"docs/ai_integrations",slug:"/docs/ai_integrations/transformers",permalink:"/docs/docs/ai_integrations/transformers",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/blob/main/docs/hr/content/docs/ai_integrations/transformers.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Scikit-learn",permalink:"/docs/docs/ai_integrations/sklearn"},next:{title:"Fundamentals",permalink:"/docs/docs/fundamentals/overview"}},l={},d=[{value:"Supported <code>Model</code> types",id:"supported-model-types",level:2},{value:"<code>TextClassification</code>",id:"textclassification",level:3},{value:"<code>LLM</code>",id:"llm",level:3},{value:"Training",id:"training",level:2},{value:"LLM fine-tuning",id:"llm-fine-tuning",level:3},{value:"Supported Features",id:"supported-features",level:3},{value:"Training Configuration",id:"training-configuration",level:3},{value:"Define The Model",id:"define-the-model",level:4},{value:"Define Training Parameter Configuration",id:"define-training-parameter-configuration",level:4},{value:"Execute Training",id:"execute-training",level:4},{value:"<code>on_ray</code>",id:"on_ray",level:5},{value:"<code>ray_address</code>",id:"ray_address",level:5},{value:"<code>ray_configs</code>",id:"ray_configs",level:5}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"transformers",children:"Transformers"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/index",children:"Transformers"})," is a popular AI framework, and we have incorporated native support for Transformers to provide essential Large Language Model (LLM) capabilities.\n",(0,i.jsx)(n.code,{children:"superduperdb"})," allows users to work with arbitrary ",(0,i.jsx)(n.code,{children:"transformers"})," pipelines, with custom input/ output data-types."]}),"\n",(0,i.jsxs)(n.h2,{id:"supported-model-types",children:["Supported ",(0,i.jsx)(n.code,{children:"Model"})," types"]}),"\n",(0,i.jsx)(n.h3,{id:"textclassification",children:(0,i.jsx)(n.code,{children:"TextClassification"})}),"\n",(0,i.jsx)(n.p,{children:"..."}),"\n",(0,i.jsx)(n.h3,{id:"llm",children:(0,i.jsx)(n.code,{children:"LLM"})}),"\n",(0,i.jsx)(n.p,{children:"You can quickly utilize LLM capabilities using the following Python function:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from superduperdb.ext.transformers import LLM\nllm = LLM(model_name_or_path="facebook/opt-350m")\nllm.predict_one("What are we having for dinner?")\n'})}),"\n",(0,i.jsx)(n.p,{children:"The model can be configured with the following parameters:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"bits: quantization bits, ranging from 4 to 8; the default is None."}),"\n",(0,i.jsx)(n.li,{children:"adapter_id: Add an adapter to the base model for inference."}),"\n",(0,i.jsx)(n.li,{children:"model_kwargs: a dictionary; all the model_kwargs will be passed to transformers.AutoModelForCausalLM.from_pretrained. You can provide parameters such as trust_remote_code=True."}),"\n",(0,i.jsx)(n.li,{children:"tokenizer_kwargs: a dictionary; all the tokenizer_kwargs will be passed to transformers.AutoTokenizer.from_pretrained."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"training",children:"Training"}),"\n",(0,i.jsx)(n.h3,{id:"llm-fine-tuning",children:"LLM fine-tuning"}),"\n",(0,i.jsx)(n.p,{children:"SuperduperDB currently offers convenient support for model fine-tuning."}),"\n",(0,i.jsx)(n.p,{children:"We can quickly run a fine-tuning example using the qlora finetune Mistral-7B model."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Install Dependencies"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install transformers torch datasets peft bitsandbytes\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Training Script"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\nfrom superduperdb.base.document import Document\nfrom superduperdb.ext.llm import LLM\nfrom superduperdb.ext.llm.model import LLMTrainingConfiguration\n\nfrom datasets import load_dataset\n\nmodel = "mistralai/Mistral-7B-v0.1"\ndataset_name = "timdettmers/openassistant-guanaco"\n\ndb = superduper("mongomock://test_llm")\ndataset = load_dataset(dataset_name)\ntrain_dataset = dataset["train"]\neval_dataset = dataset["test"]\n\ntrain_documents = [\n    Document({"text": example["text"], "_fold": "train"})\n    for example in train_dataset\n]\neval_documents = [\n    Document({"text": example["text"], "_fold": "valid"})\n    for example in eval_dataset\n]\n\ndb.execute(Collection("datas").insert_many(train_documents))\ndb.execute(Collection("datas").insert_many(eval_documents))\n\ntrainer = LLMTrainer(\n    select=Collection("datas").find(),\n    identifier="llm-finetune-training-config",\n    output_dir="output/llm-finetune",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    save_total_limit=5,\n    logging_steps=10,\n    evaluation_strategy="steps",\n    fp16=True,\n    eval_steps=100,\n    save_steps=100,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_length=512,\n    use_lora=True,\n)\n\nllm = LLM(\n    identifier="llm-finetune",\n    bits=4,\n    model_name_or_path=model,\n    trainer=trainer,\n)\n\ndb.apply(llm)\n\nprompt = "### Human: Who are you? ### Assistant: "\n\n# Automatically load lora model for prediction, default use the latest checkpoint\nprint(llm.predict(prompt, max_new_tokens=100, do_sample=True))\n'})}),"\n",(0,i.jsxs)(n.p,{children:["This script can be found in ",(0,i.jsx)(n.a,{href:"https://github.com/SuperDuperDB/superduperdb/blob/main/examples/llm_finetune.py",children:(0,i.jsx)(n.code,{children:"llm_finetune.py"})}),"."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Running Training"}),"\nYou can execute training by running ",(0,i.jsx)(n.code,{children:"python examples/llm_finetune.py"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"If you have multiple GPUs, it will automatically use Ray for multi-GPU training."}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["If you encounter ",(0,i.jsx)(n.code,{children:"ImportError: cannot import name 'ExtensionArrayFormatter' from 'pandas.io.formats.format'"})," while using multiple GPUs, please downgrade the Pandas version with the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"pip install 'pandas<=2.1.4'\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Usage"}),"\nApart from directly loading and using the model at the end of the script, you can also use your model in other programs provided that you are connected to a real database rather than a mock database."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'llm = db.load("model", "llm-finetune")\nprompt = "### Human: Who are you? ### Assistant: "\nprint(llm.predict_one(prompt, max_new_tokens=100, do_sample=True))\n'})}),"\n",(0,i.jsx)(n.h3,{id:"supported-features",children:"Supported Features"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Training Methods"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Full fine-tuning"}),"\n",(0,i.jsx)(n.li,{children:"LoRA fine-tuning"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Parallel Training"}),":"]}),"\n",(0,i.jsxs)(n.p,{children:["Parallel training is supported using Ray, with data parallelism as the default strategy. You can also pass DeepSpeed parameters to configure parallelism through ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/main_classes/deepspeed#zero",children:"DeepSpeed configuration"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Multi-GPUs fine-tuning"}),"\n",(0,i.jsx)(n.li,{children:"Multi-nodes fine-tuning"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Remote Training"}),":\nYou can perform remote training by providing a ",(0,i.jsx)(n.code,{children:"ray_address"}),". Imagine you have a Ray cluster with GPUs, you can connect to it from your local machine for training."]}),"\n",(0,i.jsx)(n.h3,{id:"training-configuration",children:"Training Configuration"}),"\n",(0,i.jsx)(n.p,{children:"The training process consists of the following steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Define a model."}),"\n",(0,i.jsx)(n.li,{children:"Define training parameter configurations."}),"\n",(0,i.jsx)(n.li,{children:"Execute training."}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"define-the-model",children:"Define The Model"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'llm = LLM(\n    identifier="llm-finetune",\n    bits=4,\n    model_name_or_path=model,\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"LLM class model definition can be found in the above introduction."}),"\n",(0,i.jsx)(n.h4,{id:"define-training-parameter-configuration",children:"Define Training Parameter Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'training_configuration = LLMTrainingConfiguration(\n    identifier="llm-finetune-training-config",\n    output_dir="output/llm-finetune",\n    ...\n)\n'})}),"\n",(0,i.jsxs)(n.p,{children:["The configuration inherits from Huggingface ",(0,i.jsx)(n.code,{children:"transformers.TrainingArguments"}),", which means theoretically you can use any parameters supported by it."]}),"\n",(0,i.jsx)(n.p,{children:"Additionally, some extra parameters are provided to support LLM fine-tuning scenarios."}),"\n",(0,i.jsx)(n.h4,{id:"execute-training",children:"Execute Training"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'llm.fit(\n    X="text",\n    select=Collection("datas").find(),\n    configuration=training_configuration,\n    db=db,\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"By default, training will execute directly. However, if multiple GPUs are detected, training will be managed and performed in parallel using Ray."}),"\n",(0,i.jsx)(n.p,{children:"Additionally, you can manually configure Ray for training, either locally or on a remote Ray cluster."}),"\n",(0,i.jsxs)(n.p,{children:["Provide three ",(0,i.jsx)(n.code,{children:"ray"}),"-related parameters for configuration:"]}),"\n",(0,i.jsx)(n.h5,{id:"on_ray",children:(0,i.jsx)(n.code,{children:"on_ray"})}),"\n",(0,i.jsx)(n.p,{children:"Whether to perform training on Ray."}),"\n",(0,i.jsx)(n.h5,{id:"ray_address",children:(0,i.jsx)(n.code,{children:"ray_address"})}),"\n",(0,i.jsx)(n.p,{children:"The address of the Ray cluster to connect to. If not provided, a Ray service will be started locally by default."}),"\n",(0,i.jsx)(n.h5,{id:"ray_configs",children:(0,i.jsx)(n.code,{children:"ray_configs"})}),"\n",(0,i.jsxs)(n.p,{children:["All ray_configs will be passed to ",(0,i.jsxs)(n.a,{href:"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.TorchTrainer.html",children:["ray's ",(0,i.jsx)(n.code,{children:"TorchTrainer"})]}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Except for the following three fields, which are automatically built by SuperDuperDB:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"train_loop_per_worker"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"train_loop_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"datasets"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"For example, you can provide a configuration like this:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from ray.train import RunConfig, ScalingConfig\n\nscaling_config = ScalingConfig(\n    num_workers=4, # Number of GPUs you need\n    use_gpu=True,\n)\n\nrun_config = RunConfig(\n    storage_path="s3://llm-test/llm-finetune",\n    name="llm-finetune-test100",\n)\n\nray_configs = {\n    "scaling_config": scaling_config,\n    "run_config": run_config,\n}\n\ntrainer = LLMTrainer(\n    ...,\n    ray_configs=ray_configs,\n    on_ray=True\n)\n\ndb.apply(\n    LLM(\n        ...,\n        trainer=trainer,\n    )\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"For information on how to configure Ray resources, please refer to the ray documentation, such as:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html#ray.train.ScalingConfig",children:"ScalingConfig"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html#ray.train.RunConfig",children:"RunConfig"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>s});var i=r(6540);const t={},a=i.createContext(t);function o(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);