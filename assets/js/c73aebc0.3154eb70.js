"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[3122],{47544:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>d});var i=t(85893),o=t(11151);const r={},s="MNIST in Database",a={id:"use_cases/mnist_torch",title:"MNIST in Database",description:"Training and Maintaining MNIST Predictions with SuperDuperDB",source:"@site/content/use_cases/mnist_torch.md",sourceDirName:"use_cases",slug:"/use_cases/mnist_torch",permalink:"/docs/use_cases/mnist_torch",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/mnist_torch.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"QA Assistant & RAG",permalink:"/docs/use_cases/chatbot/"},next:{title:"Image Feature Extraction",permalink:"/docs/use_cases/resnet_features"}},c={},d=[{value:"Training and Maintaining MNIST Predictions with SuperDuperDB",id:"training-and-maintaining-mnist-predictions-with-superduperdb",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Build Model",id:"build-model",level:2},{value:"Train Model",id:"train-model",level:2},{value:"Monitoring Training Efficiency",id:"monitoring-training-efficiency",level:2},{value:"On-the-fly Predictions",id:"on-the-fly-predictions",level:2},{value:"Verification",id:"verification",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"mnist-in-database",children:"MNIST in Database"}),"\n",(0,i.jsx)(n.h2,{id:"training-and-maintaining-mnist-predictions-with-superduperdb",children:"Training and Maintaining MNIST Predictions with SuperDuperDB"}),"\n",(0,i.jsx)(n.p,{children:"This notebook outlines the process of implementing a classic machine learning classification task - MNIST handwritten digit recognition, using a convolutional neural network. However, we introduce a unique twist by performing the task in a database using SuperDuperDB."}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"!pip install superduperdb\n!pip install torch torchvision matplotlib\n"})}),"\n",(0,i.jsx)(n.h2,{id:"connect-to-datastore",children:"Connect to datastore"}),"\n",(0,i.jsxs)(n.p,{children:["First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,i.jsx)(n.code,{children:"MongoDB_URI"})," based on your specific setup.\nHere are some examples of MongoDB URIs:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["For testing (default connection): ",(0,i.jsx)(n.code,{children:"mongomock://test"})]}),"\n",(0,i.jsxs)(n.li,{children:["Local MongoDB instance: ",(0,i.jsx)(n.code,{children:"mongodb://localhost:27017"})]}),"\n",(0,i.jsxs)(n.li,{children:["MongoDB with authentication: ",(0,i.jsx)(n.code,{children:"mongodb://superduper:superduper@mongodb:27017/documents"})]}),"\n",(0,i.jsxs)(n.li,{children:["MongoDB Atlas: ",(0,i.jsx)(n.code,{children:"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"})]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\nimport os\n\nmongodb_uri = os.getenv("MONGODB_URI","mongomock://test")\ndb = superduper(mongodb_uri)\n\n# Create a collection for MNIST\nmnist_collection = Collection(\'mnist\')\n'})}),"\n",(0,i.jsx)(n.h2,{id:"load-dataset",children:"Load Dataset"}),"\n",(0,i.jsxs)(n.p,{children:['After connecting to MongoDB, we add the MNIST dataset. SuperDuperDB excels at handling "difficult" data types, and we achieve this using an ',(0,i.jsx)(n.code,{children:"Encoder"}),", which works in tandem with the ",(0,i.jsx)(n.code,{children:"Document"})," wrappers. Together, they enable Python dictionaries containing non-JSONable or bytes objects to be inserted into the underlying data infrastructure."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torchvision\nfrom superduperdb.ext.pillow import pil_image\nfrom superduperdb import Document\nfrom superduperdb.backends.mongodb import Collection\n\nimport random\n\n# Load MNIST images as Python objects using the Python Imaging Library.\nmnist_data = list(torchvision.datasets.MNIST(root='./data', download=True))\ndocument_list = [Document({'img': pil_image(x[0]), 'class': x[1]}) for x in mnist_data]\n\n# Shuffle the data and select a subset of 1000 documents\nrandom.shuffle(document_list)\ndata = document_list[:1000]\n\n# Insert the selected data into the mnist_collection\ndb.execute(\n    mnist_collection.insert_many(data[:-100]),  # Insert all but the last 100 documents\n    encoders=(pil_image,) # Encode images using the Pillow library.\n)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Now that the images and their classes are inserted into the database, we can query the data in its original format. Particularly, we can use the ",(0,i.jsx)(n.code,{children:"PIL.Image"})," instances to inspect the data."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Get and display one of the images\nr = db.execute(mnist_collection.find_one())\nr.unpack()['img']\n"})}),"\n",(0,i.jsx)(n.h2,{id:"build-model",children:"Build Model"}),"\n",(0,i.jsx)(n.p,{children:"Next, we create our machine learning model. SuperDuperDB supports various frameworks out of the box, and in this case, we are using PyTorch, which is well-suited for computer vision tasks. In this example, we combine torch with torchvision."}),"\n",(0,i.jsxs)(n.p,{children:["We create ",(0,i.jsx)(n.code,{children:"postprocess"})," and ",(0,i.jsx)(n.code,{children:"preprocess"})," functions to handle the communication with the SuperDuperDB ",(0,i.jsx)(n.code,{children:"Datalayer"}),", and then wrap model, preprocessing and postprocessing to create a native SuperDuperDB handler."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\n\nclass LeNet5(torch.nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n            torch.nn.BatchNorm2d(6),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n            torch.nn.BatchNorm2d(16),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = torch.nn.Linear(400, 120)\n        self.relu = torch.nn.ReLU()\n        self.fc1 = torch.nn.Linear(120, 84)\n        self.relu1 = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(84, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        out = self.relu(out)\n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        return out\n\n    \ndef postprocess(x):\n    return int(x.topk(1)[1].item())\n\n\ndef preprocess(x):\n    return torchvision.transforms.Compose([\n        torchvision.transforms.Resize((32, 32)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean=(0.1307,), std=(0.3081,))]\n    )(x)\n\n\n# Create and insert a SuperDuperDB model into the database\nmodel = superduper(LeNet5(10), preprocess=preprocess, postprocess=postprocess, preferred_devices=('cpu',))\ndb.add(model)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"train-model",children:"Train Model"}),"\n",(0,i.jsxs)(n.p,{children:['Now we are ready to "train" or "fit" the model. Trainable models in SuperDuperDB come with a sklearn-like ',(0,i.jsx)(n.code,{children:".fit"})," method."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from torch.nn.functional import cross_entropy\n\nfrom superduperdb import Metric\nfrom superduperdb import Dataset\nfrom superduperdb.ext.torch.model import TorchTrainerConfiguration\n\n# Fit the model to the training data\njob = model.fit(\n    X='img', # Feature matrix used as input data \n    y='class', # Target variable for training\n    db=db, # Database used for data retrieval\n    select=mnist_collection.find(), # Select the dataset\n    configuration=TorchTrainerConfiguration(\n        identifier='my_configuration',\n        objective=cross_entropy,\n        loader_kwargs={'batch_size': 10},\n        max_iterations=10,\n        validation_interval=5,\n    ),\n    metrics=[Metric(identifier='acc', object=lambda x, y: sum([xx == yy for xx, yy in zip(x, y)]) / len(x))],\n    validation_sets=[\n        Dataset(\n            identifier='my_valid',\n            select=Collection('mnist').find({'_fold': 'valid'}),\n        )\n    ],\n    distributed=False,\n)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-training-efficiency",children:"Monitoring Training Efficiency"}),"\n",(0,i.jsx)(n.p,{children:"You can monitor the training efficiency with visualization tools like Matplotlib:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from matplotlib import pyplot as plt\n\n# Load the model from the database\nmodel = db.load('model', model.identifier)\n\n# Plot the accuracy values\nplt.plot(model.metric_values['my_valid/acc'])\nplt.show()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"on-the-fly-predictions",children:"On-the-fly Predictions"}),"\n",(0,i.jsxs)(n.p,{children:["Once the model is trained, you can use it to continuously predict on new data as it arrives. This is set up by enabling a ",(0,i.jsx)(n.code,{children:"listener"})," for the database (without loading all the data client-side). The listen toggle activates the model to make predictions on incoming data changes."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"model.predict(\n    X='img', # Input feature  \n    db=db,  # Database used for data retrieval\n    select=mnist_collection.find(), # Select the dataset\n    listen=True, # Continuous predictions on incoming data \n    max_chunk_size=100, # Number of predictions to return at once\n)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["We can see that predictions are available in ",(0,i.jsx)(n.code,{children:"_outputs.img.lenet5"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"r = db.execute(mnist_collection.find_one({'_fold': 'valid'}))\nr.unpack()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"verification",children:"Verification"}),"\n",(0,i.jsx)(n.p,{children:'The models "activated" can be seen here:'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"db.show('listener')\n"})}),"\n",(0,i.jsx)(n.p,{children:"We can verify that the model is activated, by inserting the rest of the data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"for r in data[-100:]:\n    r['update'] = True\n\ndb.execute(mnist_collection.insert_many(data[-100:]))\n"})}),"\n",(0,i.jsx)(n.p,{children:"You can see that the inserted data, are now also populated with predictions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"db.execute(mnist_collection.find_one({'update': True}))['_outputs']\n"})})]})}function u(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>s});var i=t(67294);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);