"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[1346],{2053:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>o,toc:()=>d});var i=r(4848),s=r(8453);const t={},a="Transformers",o={id:"docs/ai_integrations/transformers",title:"Transformers",description:"Transformers is a popular AI framework, and we have incorporated native support for Transformers to provide essential Large Language Model (LLM) capabilities.",source:"@site/content/docs/ai_integrations/transformers.md",sourceDirName:"docs/ai_integrations",slug:"/docs/ai_integrations/transformers",permalink:"/docs/docs/ai_integrations/transformers",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/blob/main/docs/hr/content/docs/ai_integrations/transformers.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Scikit-learn",permalink:"/docs/docs/ai_integrations/sklearn"},next:{title:"vLLM",permalink:"/docs/docs/ai_integrations/vllm"}},l={},d=[{value:"Supported <code>Model</code> types",id:"supported-model-types",level:2},{value:"<code>TextClassification</code>",id:"textclassification",level:3},{value:"<code>LLM</code>",id:"llm",level:3},{value:"Training",id:"training",level:2},{value:"LLM fine-tuning",id:"llm-fine-tuning",level:3},{value:"Supported Features",id:"supported-features",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"transformers",children:"Transformers"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/index",children:"Transformers"})," is a popular AI framework, and we have incorporated native support for Transformers to provide essential Large Language Model (LLM) capabilities.\n",(0,i.jsx)(n.code,{children:"superduperdb"})," allows users to work with arbitrary ",(0,i.jsx)(n.code,{children:"transformers"})," pipelines, with custom input/ output data-types."]}),"\n",(0,i.jsxs)(n.h2,{id:"supported-model-types",children:["Supported ",(0,i.jsx)(n.code,{children:"Model"})," types"]}),"\n",(0,i.jsx)(n.h3,{id:"textclassification",children:(0,i.jsx)(n.code,{children:"TextClassification"})}),"\n",(0,i.jsx)(n.p,{children:"..."}),"\n",(0,i.jsx)(n.h3,{id:"llm",children:(0,i.jsx)(n.code,{children:"LLM"})}),"\n",(0,i.jsx)(n.p,{children:"You can quickly utilize LLM capabilities using the following Python function:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from superduperdb.ext.transformers import LLM\nllm = LLM(model_name_or_path="facebook/opt-350m")\nllm.predict_one("What are we having for dinner?")\n'})}),"\n",(0,i.jsx)(n.p,{children:"Or use a method similar to transformers\u2019 from_pretrained, just need to supplement the identifier parameter."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from superduperdb.ext.transformers import LLM\nllm = LLM.from_pretrained(\n    "facebook/opt-350m", \n    load_in_8bit=True, \n    device_map="cuda", \n    identifier="llm",\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"The model can be configured with the following parameters:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"adapter_id: Add an adapter to the base model for inference."}),"\n",(0,i.jsx)(n.li,{children:"model_kwargs: a dictionary; all the model_kwargs will be passed to transformers.AutoModelForCausalLM.from_pretrained. You can provide parameters such as trust_remote_code=True."}),"\n",(0,i.jsx)(n.li,{children:"tokenizer_kwargs: a dictionary; all the tokenizer_kwargs will be passed to transformers.AutoTokenizer.from_pretrained."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"training",children:"Training"}),"\n",(0,i.jsx)(n.h3,{id:"llm-fine-tuning",children:"LLM fine-tuning"}),"\n",(0,i.jsxs)(n.p,{children:["SuperDuperDB provides a convenient fine-tuning method based on the ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/trl/index",children:"trl"})," framework to help you train data in the database."]}),"\n",(0,i.jsxs)(n.p,{children:["You can find ",(0,i.jsx)(n.a,{href:"../../use_cases/fine_tune_llm_on_database",children:"training examples here"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Running Training"})}),"\n",(0,i.jsx)(n.h3,{id:"supported-features",children:"Supported Features"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Training Methods"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Full fine-tuning"}),"\n",(0,i.jsx)(n.li,{children:"LoRA fine-tuning"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Parallel Training"}),":"]}),"\n",(0,i.jsxs)(n.p,{children:["Parallel training is supported using Ray, with data parallelism as the default strategy. You can also pass DeepSpeed parameters to configure parallelism through ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/main_classes/deepspeed#zero",children:"DeepSpeed configuration"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Multi-GPUs fine-tuning"}),"\n",(0,i.jsx)(n.li,{children:"Multi-nodes fine-tuning"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Training on Ray"}),":"]}),"\n",(0,i.jsx)(n.p,{children:"We can use Ray to train models. When using Ray as the compute backend, tasks will automatically run in Ray and the program will no longer be blocked."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var i=r(6540);const s={},t=i.createContext(s);function a(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);