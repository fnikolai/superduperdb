"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[744],{48683:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>d,toc:()=>c});var i=t(85893),o=t(11151);const r={},s="Search within videos with text",d={id:"use_cases/items/video_search",title:"Search within videos with text",description:"Introduction",source:"@site/content/use_cases/items/video_search.md",sourceDirName:"use_cases/items",slug:"/use_cases/items/video_search",permalink:"/docs/use_cases/items/video_search",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/items/video_search.md",tags:[],version:"current",frontMatter:{},sidebar:"useCasesSidebar",previous:{title:"Vector-search with SuperDuperDB",permalink:"/docs/use_cases/items/vector_search"},next:{title:"Cataloguing voice-memos for a self managed personal assistant",permalink:"/docs/use_cases/items/voice_memos"}},a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Register Encoders",id:"register-encoders",level:2},{value:"Create CLIP model",id:"create-clip-model",level:2},{value:"Create VectorIndex",id:"create-vectorindex",level:2},{value:"Query a text against saved frames.",id:"query-a-text-against-saved-frames",level:2},{value:"Start the video from the resultant timestamp:",id:"start-the-video-from-the-resultant-timestamp",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"search-within-videos-with-text",children:"Search within videos with text"}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"This notebook outlines the process of searching for specific textual information within videos and retrieving relevant video segments. To accomplish this, we utilize various libraries and techniques, such as:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"clip: A library for vision and language understanding."}),"\n",(0,i.jsx)(n.li,{children:"PIL: Python Imaging Library for image processing."}),"\n",(0,i.jsx)(n.li,{children:"torch: The PyTorch library for deep learning."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"!pip install superduperdb\n!pip install ipython opencv-python pillow openai-clip\n"})}),"\n",(0,i.jsx)(n.h2,{id:"connect-to-datastore",children:"Connect to datastore"}),"\n",(0,i.jsxs)(n.p,{children:["First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,i.jsx)(n.code,{children:"MongoDB_URI"})," based on your specific setup.\nHere are some examples of MongoDB URIs:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["For testing (default connection): ",(0,i.jsx)(n.code,{children:"mongomock://test"})]}),"\n",(0,i.jsxs)(n.li,{children:["Local MongoDB instance: ",(0,i.jsx)(n.code,{children:"mongodb://localhost:27017"})]}),"\n",(0,i.jsxs)(n.li,{children:["MongoDB with authentication: ",(0,i.jsx)(n.code,{children:"mongodb://superduper:superduper@mongodb:27017/documents"})]}),"\n",(0,i.jsxs)(n.li,{children:["MongoDB Atlas: ",(0,i.jsx)(n.code,{children:"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"})]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\nfrom superduperdb import CFG\nimport os\n\nCFG.downloads.hybrid = True\nCFG.downloads.root = './'\n\nmongodb_uri = os.getenv(\"MONGODB_URI\",\"mongomock://test\")\ndb = superduper(mongodb_uri, artifact_store='filesystem://./data/')\n\nvideo_collection = Collection('videos')\n"})}),"\n",(0,i.jsx)(n.h2,{id:"load-dataset",children:"Load Dataset"}),"\n",(0,i.jsx)(n.p,{children:"We'll begin by configuring a video encoder."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from superduperdb import Encoder\n\nvid_enc = Encoder(\n    identifier='video_on_file',\n    load_hybrid=False,\n)\n\ndb.add(vid_enc)\n"})}),"\n",(0,i.jsx)(n.p,{children:"Now, let's retrieve a sample video from the internet and insert it into our collection."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from superduperdb.base.document import Document\n\ndb.execute(video_collection.insert_one(\n        Document({'video': vid_enc(uri='https://superduperdb-public.s3.eu-west-1.amazonaws.com/animals_excerpt.mp4')})\n    )\n)\n\n# Display the list of videos in the collection\nlist(db.execute(Collection('videos').find()))\n"})}),"\n",(0,i.jsx)(n.h2,{id:"register-encoders",children:"Register Encoders"}),"\n",(0,i.jsx)(n.p,{children:"Next, we'll create encoders for processing videos and extracting frames. This encoder will help us convert videos into individual frames."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\nimport tqdm\nfrom PIL import Image\nfrom superduperdb.ext.pillow import pil_image\nfrom superduperdb import Model, Schema\n\n\ndef video2images(video_file):\n    sample_freq = 10\n    cap = cv2.VideoCapture(video_file)\n\n    frame_count = 0\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    print(fps)\n    extracted_frames = []\n    progress = tqdm.tqdm()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        current_timestamp = frame_count // fps\n        \n        if frame_count % sample_freq == 0:\n            extracted_frames.append({\n                'image': Image.fromarray(frame[:,:,::-1]),\n                'current_timestamp': current_timestamp,\n            })\n        frame_count += 1        \n        progress.update(1)\n    \n    cap.release()\n    cv2.destroyAllWindows()\n    return extracted_frames\n\n\nvideo2images = Model(\n    identifier='video2images',\n    object=video2images,\n    flatten=True,\n    model_update_kwargs={'document_embedded': False},\n    output_schema=Schema(identifier='myschema', fields={'image': pil_image})\n)\n"})}),"\n",(0,i.jsx)(n.p,{children:"We'll also set up a listener to continuously download video URLs and save the best frames into another collection."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from superduperdb import Listener\n\ndb.add(\n   Listener(\n       model=video2images,\n       select=video_collection.find(),\n       key='video',\n   )\n)\n\ndb.execute(Collection('_outputs.video.video2images').find_one()).unpack()['_outputs']['video']['video2images']['image']\n"})}),"\n",(0,i.jsx)(n.h2,{id:"create-clip-model",children:"Create CLIP model"}),"\n",(0,i.jsx)(n.p,{children:"Now, we'll create a model for the CLIP (Contrastive Language-Image Pre-training) model, which will be used for visual and textual analysis."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import clip\nfrom superduperdb import vector\nfrom superduperdb.ext.torch import TorchModel\n\nmodel, preprocess = clip.load(\"RN50\", device='cpu')\nt = vector(shape=(1024,))\n\nvisual_model = TorchModel(\n    identifier='clip_image',\n    preprocess=preprocess,\n    object=model.visual,\n    encoder=t,\n    postprocess=lambda x: x.tolist(),\n)\n\ntext_model = TorchModel(\n    identifier='clip_text',\n    object=model,\n    preprocess=lambda x: clip.tokenize(x)[0],\n    forward_method='encode_text',\n    encoder=t,\n    device='cpu',\n    preferred_devices=None,\n    postprocess=lambda x: x.tolist(),\n)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"create-vectorindex",children:"Create VectorIndex"}),"\n",(0,i.jsx)(n.p,{children:"We will set up a VectorIndex to index and search the video frames based on both visual and textual content. This involves creating an indexing listener for visual data and a compatible listener for textual data."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from superduperdb import Listener, VectorIndex\nfrom superduperdb.backends.mongodb import Collection\n\ndb.add(\n    VectorIndex(\n        identifier='video_search_index',\n        indexing_listener=Listener(\n            model=visual_model,\n            key='_outputs.video.video2images.image',\n            select=Collection('_outputs.video.video2images').find(),\n        ),\n        compatible_listener=Listener(\n            model=text_model,\n            key='text',\n            select=None,\n            active=False\n        )\n    )\n)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"query-a-text-against-saved-frames",children:"Query a text against saved frames."}),"\n",(0,i.jsx)(n.p,{children:"Now, let's search for something that happened during the video:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Define the search parameters\nsearch_term = 'Some ducks'\nnum_results = 1\n\n\nr = next(db.execute(\n    Collection('_outputs.video.video2images').like(Document({'text': search_term}), vector_index='video_search_index', n=num_results).find()\n))\n\nsearch_timestamp = r['_outputs']['video']['video2images']['current_timestamp']\n\n# Get the back reference to the original video\nvideo = db.execute(Collection('videos').find_one({'_id': r['_source']}))\n"})}),"\n",(0,i.jsx)(n.h2,{id:"start-the-video-from-the-resultant-timestamp",children:"Start the video from the resultant timestamp:"}),"\n",(0,i.jsx)(n.p,{children:"Finally, we can display and play the video starting from the timestamp where the searched text is found."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from IPython.display import display, HTML\n\nvideo_html = f"""\n<video width="640" height="480" controls>\n    <source src="{video[\'video\'].uri}" type="video/mp4">\n</video>\n<script>\n    var video = document.querySelector(\'video\');\n    video.currentTime = {search_timestamp};\n    video.play();\n<\/script>\n"""\n\ndisplay(HTML(video_html))\n'})})]})}function p(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>d,a:()=>s});var i=t(67294);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);