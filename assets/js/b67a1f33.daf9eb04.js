"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[8172],{76018:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var t=s(85893),i=s(11151);const o={sidebar_position:2},r="Image",a={id:"use_cases/vector_search/multimodal_image_search_clip",title:"Image",description:"Multimodal Search Using CLIP",source:"@site/content/use_cases/vector_search/multimodal_image_search_clip.md",sourceDirName:"use_cases/vector_search",slug:"/use_cases/vector_search/multimodal_image_search_clip",permalink:"/docs/use_cases/vector_search/multimodal_image_search_clip",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/vector_search/multimodal_image_search_clip.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Text",permalink:"/docs/use_cases/vector_search/plain_vector_search"},next:{title:"Video",permalink:"/docs/use_cases/vector_search/video_search"}},c={},l=[{value:"Multimodal Search Using CLIP",id:"multimodal-search-using-clip",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Build Models",id:"build-models",level:2},{value:"Create a Vector-Search Index",id:"create-a-vector-search-index",level:2},{value:"Search Images Using Text",id:"search-images-using-text",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"image",children:"Image"}),"\n",(0,t.jsx)(n.h2,{id:"multimodal-search-using-clip",children:"Multimodal Search Using CLIP"}),"\n",(0,t.jsxs)(n.p,{children:["This notebook showcases the capabilities of SuperDuperDB for performing multimodal searches using the ",(0,t.jsx)(n.code,{children:"VectorIndex"}),". SuperDuperDB's flexibility enables users and developers to integrate various models into the system and use them for vectorizing diverse queries during search and inference. In this demonstration, we leverage the ",(0,t.jsx)(n.a,{href:"https://openai.com/research/clip",children:"CLIP multimodal architecture"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!pip install superduperdb\n!pip install ipython openai-clip\n!pip install -U datasets\n"})}),"\n",(0,t.jsx)(n.h2,{id:"connect-to-datastore",children:"Connect to datastore"}),"\n",(0,t.jsxs)(n.p,{children:["First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,t.jsx)(n.code,{children:"MongoDB_URI"})," based on your specific setup.\nHere are some examples of MongoDB URIs:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For testing (default connection): ",(0,t.jsx)(n.code,{children:"mongomock://test"})]}),"\n",(0,t.jsxs)(n.li,{children:["Local MongoDB instance: ",(0,t.jsx)(n.code,{children:"mongodb://localhost:27017"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB with authentication: ",(0,t.jsx)(n.code,{children:"mongodb://superduper:superduper@mongodb:27017/documents"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB Atlas: ",(0,t.jsx)(n.code,{children:"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\nfrom superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\n\nmongodb_uri = os.getenv(\"MONGODB_URI\", \"mongomock://test\")\ndb = superduper(mongodb_uri, artifact_store='filesystem://./models/')\n\n# Super-Duper your Database!\ndb = superduper(mongodb_uri, artifact_store='filesystem://.data')\n\ncollection = Collection('multimodal')\n"})}),"\n",(0,t.jsx)(n.h2,{id:"load-dataset",children:"Load Dataset"}),"\n",(0,t.jsxs)(n.p,{children:["To make this notebook easily executable and interactive, we'll work with a sub-sample of the ",(0,t.jsx)(n.a,{href:"https://paperswithcode.com/dataset/tiny-imagenet",children:"Tiny-Imagenet dataset"}),". The processes demonstrated here can be applied to larger datasets with higher resolution images as well. For such use-cases, however, it's advisable to use a machine with a GPU, otherwise they'll be some significant thumb twiddling to do."]}),"\n",(0,t.jsxs)(n.p,{children:["To insert images into the database, we utilize the ",(0,t.jsx)(n.code,{children:"Encoder"}),"-",(0,t.jsx)(n.code,{children:"Document"})," framework, which allows saving Python class instances as blobs in the ",(0,t.jsx)(n.code,{children:"Datalayer"})," and retrieving them as Python objects. To this end, SuperDuperDB contains pre-configured support for ",(0,t.jsx)(n.code,{children:"PIL.Image"})," instances. This simplifies the integration of Python AI models with the datalayer. It's also possible to create your own encoders."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!curl -O https://superduperdb-public.s3.eu-west-1.amazonaws.com/coco_sample.zip\n!unzip coco_sample.zip\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import Document\nfrom superduperdb.ext.pillow import pil_image as i\nimport glob\nimport random\n\nimages = glob.glob('images_small/*.jpg')\ndocuments = [Document({'image': i(uri=f'file://{img}')}) for img in images][:500]\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"documents[1]\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The wrapped python dictionaries may be inserted directly to the ",(0,t.jsx)(n.code,{children:"Datalayer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"db.execute(collection.insert_many(documents), encoders=(i,))\n"})}),"\n",(0,t.jsx)(n.p,{children:"You can verify that the images are correctly stored as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"x = db.execute(imagenet_collection.find_one()).unpack()['image']\ndisplay(x.resize((300, 300 * int(x.size[1] / x.size[0]))))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"build-models",children:"Build Models"}),"\n",(0,t.jsx)(n.p,{children:"We now can wrap the CLIP model, to ready it for multimodal search. It involves 2 components:"}),"\n",(0,t.jsxs)(n.p,{children:["Now, let's prepare the CLIP model for multimodal search, which involves two components: ",(0,t.jsx)(n.code,{children:"text encoding"})," and ",(0,t.jsx)(n.code,{children:"visual encoding"}),". After installing both components, you can perform searches using both images and text to find matching items:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import clip\nfrom superduperdb import vector\nfrom superduperdb.ext.torch import TorchModel\n\n# Load the CLIP model\nmodel, preprocess = clip.load(\"RN50\", device='cpu')\n\n# Define a vector\ne = vector(shape=(1024,))\n\n# Create a TorchModel for text encoding\ntext_model = TorchModel(\n    identifier='clip_text',\n    object=model,\n    preprocess=lambda x: clip.tokenize(x)[0],\n    postprocess=lambda x: x.tolist(),\n    encoder=e,\n    forward_method='encode_text',    \n)\n\n# Create a TorchModel for visual encoding\nvisual_model = TorchModel(\n    identifier='clip_image',\n    object=model.visual,    \n    preprocess=preprocess,\n    postprocess=lambda x: x.tolist(),\n    encoder=e,\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"create-a-vector-search-index",children:"Create a Vector-Search Index"}),"\n",(0,t.jsxs)(n.p,{children:["Let's create the index for vector-based searching. We'll register both models with the index simultaneously, but specify that the ",(0,t.jsx)(n.code,{children:"visual_model"})," will be responsible for creating the vectors in the database (",(0,t.jsx)(n.code,{children:"indexing_listener"}),"). The ",(0,t.jsx)(n.code,{children:"compatible_listener"})," specifies how an alternative model can be used to search the vectors, enabling multimodal search with models expecting different types of indexes."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import VectorIndex\nfrom superduperdb import Listener\n\n# Create a VectorIndex and add it to the database\ndb.add(\n    VectorIndex(\n        'my-index',\n        indexing_listener=Listener(\n            model=visual_model,\n            key='image',\n            select=collection.find(),\n            predict_kwargs={'batch_size': 10},\n        ),\n        compatible_listener=Listener(\n            model=text_model,\n            key='text',\n            active=False,\n            select=None,\n        )\n    )\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"search-images-using-text",children:"Search Images Using Text"}),"\n",(0,t.jsx)(n.p,{children:"Now we can demonstrate searching for images using text queries:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from IPython.display import display\nfrom superduperdb import Document\n\nquery_string = 'sports'\n\nout = db.execute(\n    collection.like(Document({'text': query_string}), vector_index='my-index', n=3).find({})\n)\n\n# Display the images from the search results\nfor r in search_results:\n    x = r['image'].x\n    display(x.resize((300, int(300 * x.size[1] / x.size[0]))))\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"img = db.execute(collection.find_one({}))['image']\nimg.x\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"cur = db.execute(\n    collection.like(Document({'image': img}), vector_index='my-index', n=3).find({})\n)\n\nfor r in cur:\n    x = r['image'].x\n    display(x.resize((300, int(300 * x.size[1] / x.size[0]))))\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python"})})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},11151:(e,n,s)=>{s.d(n,{Z:()=>a,a:()=>r});var t=s(67294);const i={},o=t.createContext(i);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);