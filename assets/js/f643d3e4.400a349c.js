"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[2692],{357:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>d});var r=t(4848),s=t(8453);const i={},o="Listening for new data",a={id:"tutorials/listening",title:"Listening for new data",description:"In SuperDuperDB, AI models may be configured to listen for newly inserted data.",source:"@site/content/tutorials/listening.md",sourceDirName:"tutorials",slug:"/tutorials/listening",permalink:"/docs/tutorials/listening",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/blob/main/docs/hr/content/tutorials/listening.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Vector-search",permalink:"/docs/tutorials/vector_search"},next:{title:"Training models",permalink:"/docs/tutorials/training"}},l={},d=[];function c(e){const n={code:"code",h1:"h1",li:"li",ol:"ol",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"listening-for-new-data",children:"Listening for new data"}),"\n",(0,r.jsx)(n.p,{children:"In SuperDuperDB, AI models may be configured to listen for newly inserted data.\nOutputs will be computed over that data and saved back to the data-backend."}),"\n",(0,r.jsx)(n.p,{children:"In this example we show how to configure 3 models to interact when new data is added."}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["A featurizing computer vision model (images ",(0,r.jsx)(n.code,{children:"->"})," vectors)."]}),"\n",(0,r.jsx)(n.li,{children:"2 models evaluating image-2-text similarity to a set of key-words."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"!curl -O https://superduperdb-public-demo.s3.amazonaws.com/images.zip && unzip images.zip\nfrom PIL import Image\n\ndata = [f'images/{x}' for x in os.listdir('./images')]\ndata = [Image.open(path) for path in data]\nsample_datapoint = data[-1]\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from superduperdb import superduper\n\ndb = superduper('mongomock://')\n\ndb['images'].insert_many(data)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import torch\nimport clip\nfrom torchvision import transforms\nfrom superduperdb import ObjectModel\nfrom superduperdb import Listener\n\nimport torch\nimport clip\nfrom PIL import Image\n\n\nclass CLIPModel:\n    def __init__(self):\n        # Load the CLIP model\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.model, self.preprocess = clip.load("RN50", device=self.device)\n\n    def __call__(self, text, image):\n        with torch.no_grad():\n            text = clip.tokenize([text]).to(self.device)\n            image = self.preprocess(Image.fromarray(image.astype(np.uint8))).unsqueeze(0).to(self.device)\n            image_features = self.model.encode_image(image)[0].numpy().tolist()\n            text_features = self.model.encode_text(text)[0].numpy().tolist()\n        return [image_features, text_features]\n        \n\nmodel = ObjectModel(\n    identifier="clip",\n    object=CLIPModel(),\n    signature="**kwargs",\n)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"listener = model.to_listener(\n    select=db['images'].find(),\n    key='image',\n    identifier='image_predictions',\n)\n\ndb.apply(listener)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"words = ['hat', 'cat', 'mat']\n\ntargets = {word: model.predict_one(word) for word in words}\n\nclass Comparer:\n    def __init__(self, targets):\n        self.targets = targets\n        self.lookup = list(self.targets.keys())\n        self.matrix = torch.stack(list(self.targets.values()))\n\n    def __call__(self, vector):\n        best = (self.matrix @ vector).topk(1)[1].item()\n        return self.lookup[best]\n\ncomparer = ObjectModel(\n    'comparer',\n    object=Comparer(targets)).to_listener(\n        select=db['images'].find(), \n        key=f'_outputs.{listener.uuid}'\n    ),\n)\n\ndb.apply(comparer)\n"})})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var r=t(6540);const s={},i=r.createContext(s);function o(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);