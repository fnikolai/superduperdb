"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[953],{30135:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var t=s(85893),a=s(11151);const r={},i="Sentiment analysis with transformers",o={id:"use_cases/items/sentiment_analysis_use_case",title:"Sentiment analysis with transformers",description:"In this notebook we implement a classic NLP use-case using Hugging Face's transformers library.",source:"@site/content/use_cases/items/sentiment_analysis_use_case.md",sourceDirName:"use_cases/items",slug:"/use_cases/items/sentiment_analysis_use_case",permalink:"/docs/use_cases/items/sentiment_analysis_use_case",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/items/sentiment_analysis_use_case.md",tags:[],version:"current",frontMatter:{},sidebar:"useCasesSidebar",previous:{title:"Creating a DB of image features in torchvision",permalink:"/docs/use_cases/items/resnet_features"},next:{title:"End-2-end example using SQL databases",permalink:"/docs/use_cases/items/sql-example"}},c={},d=[];function l(e){const n={code:"code",h1:"h1",p:"p",pre:"pre",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"sentiment-analysis-with-transformers",children:"Sentiment analysis with transformers"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!pip install superduperdb==0.0.12\n!pip install datasets\n"})}),"\n",(0,t.jsxs)(n.p,{children:["In this notebook we implement a classic NLP use-case using Hugging Face's ",(0,t.jsx)(n.code,{children:"transformers"})," library.\nWe show that this use-case may be implementing directly in SuperDuperDB using MongoDB as the\ndata-backend."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from datasets import load_dataset\nimport numpy\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nfrom superduperdb import Document as D, Dataset \nfrom superduperdb.ext.transformers import TransformersTrainerConfiguration, Pipeline\n"})}),"\n",(0,t.jsx)(n.p,{children:'SuperDuperDB supports MongoDB as a databackend.\nCorrespondingly, we\'ll import the python MongoDB client pymongo and "wrap" our database to convert it\nto a SuperDuper Datalayer:'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nfrom superduperdb.backends.mongodb import Collection\n\n# Uncomment one of the following lines to use a bespoke MongoDB deployment\n# For testing the default connection is to mongomock\n\nos.environ[\'PYTORCH_ENABLE_MPS_FALLBACK\'] = \'1\'\nmongodb_uri = os.getenv("MONGODB_URI","mongomock://test")\n# mongodb_uri = "mongodb://localhost:27017"\n# mongodb_uri = "mongodb://superduper:superduper@mongodb:27017/documents"\n# mongodb_uri = "mongodb://<user>:<pass>@<mongo_cluster>/<database>"\n# mongodb_uri = "mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"\n\n# Super-Duper your Database!\nfrom superduperdb import superduper\ndb = superduper(mongodb_uri)\ncollection = Collection(\'imdb\')\n'})}),"\n",(0,t.jsx)(n.p,{children:"We use the IMDB dataset for training the model:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"data = load_dataset(\"imdb\")\n\n# increase this number to do serious training\nN_DATAPOINTS = 4\n\ndb.execute(collection.insert_many([\n    D({'_fold': 'train', **data['train'][int(i)]}) for i in numpy.random.permutation(len(data['train']))[:N_DATAPOINTS]\n]))\n\ndb.execute(collection.insert_many([\n    D({'_fold': 'valid', **data['test'][int(i)]}) for i in numpy.random.permutation(len(data['test']))[:N_DATAPOINTS]\n]))\n"})}),"\n",(0,t.jsx)(n.p,{children:"Check a sample from the database:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"r = db.execute(collection.find_one())\nr\n"})}),"\n",(0,t.jsx)(n.p,{children:"Create a tokenizer and use it to provide a data-collator for batching inputs:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\nmodel = Pipeline(\n    identifier='my-sentiment-analysis',\n    task='text-classification',\n    preprocess=tokenizer,\n    object=model,\n    preprocess_kwargs={'truncation': True},\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"model.predict('This is another test', one=True)\n"})}),"\n",(0,t.jsx)(n.p,{children:"We'll evaluate the model using a simple accuracy metric. This metric gets logged in the\nmodel's metadata during training:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"training_args = TransformersTrainerConfiguration(\n    identifier='sentiment-analysis',\n    output_dir='sentiment-analysis',\n    learning_rate=2e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    use_cpu=True,\n    evaluation_strategy='epoch',\n    do_eval=True,\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Now we're ready to train the model:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import Metric\n\nmodel.fit(\n    X='text',\n    y='label',\n    db=db,\n    select=collection.find(),\n    configuration=training_args,\n    validation_sets=[\n        Dataset(\n            identifier='my-eval',\n            select=collection.find({'_fold': 'valid'}),\n        )\n    ],\n    data_prefetch=False,\n    metrics=[Metric(\n        identifier='acc',\n        object=lambda x, y: sum([xx == yy for xx, yy in zip(x, y)]) / len(x)\n    )]\n)                                                                            \n"})}),"\n",(0,t.jsx)(n.p,{children:"We can verify that the model gives us reasonable predictions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'model.predict("This movie sucks!", one=True)\n'})})]})}function u(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},11151:(e,n,s)=>{s.d(n,{Z:()=>o,a:()=>i});var t=s(67294);const a={},r=t.createContext(a);function i(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);