"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[9270],{6666:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>a,toc:()=>d});var t=o(85893),i=o(11151);const s={},r="Voice Memo Cataloging",a={id:"use_cases/voice_memos",title:"Voice Memo Cataloging",description:"Cataloguing voice-memos for a self managed personal assistant",source:"@site/content/use_cases/voice_memos.md",sourceDirName:"use_cases",slug:"/use_cases/voice_memos",permalink:"/docs/use_cases/voice_memos",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/voice_memos.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Transfer Learning",permalink:"/docs/use_cases/transfer_learning"},next:{title:"Glossary",permalink:"/docs/docs/fundamentals/glossary"}},c={},d=[{value:"Cataloguing voice-memos for a self managed personal assistant",id:"cataloguing-voice-memos-for-a-self-managed-personal-assistant",level:2},{value:"Objectives:",id:"objectives",level:3},{value:"Our approach involves:",id:"our-approach-involves",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Install Pre-Trained Model (LibreSpeech) into Database",id:"install-pre-trained-model-librespeech-into-database",level:2},{value:"Ask Questions to Your Voice Assistant",id:"ask-questions-to-your-voice-assistant",level:2},{value:"Enrich it with Chat-Completion",id:"enrich-it-with-chat-completion",level:2},{value:"Full Voice-Assistant Experience",id:"full-voice-assistant-experience",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"voice-memo-cataloging",children:"Voice Memo Cataloging"}),"\n",(0,t.jsx)(n.h2,{id:"cataloguing-voice-memos-for-a-self-managed-personal-assistant",children:"Cataloguing voice-memos for a self managed personal assistant"}),"\n",(0,t.jsx)(n.p,{children:"Discover the magic of SuperDuperDB as we seamlessly integrate models across different data modalities, such as audio and text. Experience the creation of highly sophisticated data-based applications with minimal boilerplate code."}),"\n",(0,t.jsx)(n.h3,{id:"objectives",children:"Objectives:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Maintain a database of audio recordings"}),"\n",(0,t.jsx)(n.li,{children:"Index the content of these audio recordings"}),"\n",(0,t.jsx)(n.li,{children:"Search and interrogate the content of these audio recordings"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"our-approach-involves",children:"Our approach involves:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Utilizing a transformers model by Facebook's AI team to transcribe audio to text."}),"\n",(0,t.jsx)(n.li,{children:"Employing an OpenAI vectorization model to index the transcribed text."}),"\n",(0,t.jsx)(n.li,{children:"Harnessing OpenAI ChatGPT model in conjunction with relevant recordings to query the audio database."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!pip install superduperdb\n!pip install transformers soundfile torchaudio librosa openai\n!pip install -U datasets\n"})}),"\n",(0,t.jsx)(n.p,{children:"Additionally, ensure that you have set your openai API key as an environment variable. You can uncomment the following code and add your API key:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\n\n#os.environ['OPENAI_API_KEY'] = 'sk-XXXX'\n\nif 'OPENAI_API_KEY' not in os.environ:\n    raise Exception('Environment variable \"OPENAI_API_KEY\" not set')\n"})}),"\n",(0,t.jsx)(n.h2,{id:"connect-to-datastore",children:"Connect to datastore"}),"\n",(0,t.jsxs)(n.p,{children:["First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,t.jsx)(n.code,{children:"MongoDB_URI"})," based on your specific setup.\nHere are some examples of MongoDB URIs:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For testing (default connection): ",(0,t.jsx)(n.code,{children:"mongomock://test"})]}),"\n",(0,t.jsxs)(n.li,{children:["Local MongoDB instance: ",(0,t.jsx)(n.code,{children:"mongodb://localhost:27017"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB with authentication: ",(0,t.jsx)(n.code,{children:"mongodb://superduper:superduper@mongodb:27017/documents"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB Atlas: ",(0,t.jsx)(n.code,{children:"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\nimport os\n\nmongodb_uri = os.getenv("MONGODB_URI","mongomock://test")\ndb = superduper(mongodb_uri)\n\n# Create a collection for Voice memos\nvoice_collection = Collection(\'voice-memos\')\n'})}),"\n",(0,t.jsx)(n.h2,{id:"load-dataset",children:"Load Dataset"}),"\n",(0,t.jsxs)(n.p,{children:["In this example se use ",(0,t.jsx)(n.code,{children:"LibriSpeech"})," as our voice recording dataset. It is a corpus of approximately 1000 hours of read English speech. The same functionality could be accomplised using any audio, in particular audio hosted on the web, or in an ",(0,t.jsx)(n.code,{children:"s3"})," bucket. For instance, if you have a repository of audio of conference calls, or memos, this may be indexed in the same way."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from datasets import load_dataset\nfrom superduperdb.ext.numpy import array\nfrom superduperdb import Document\n\ndata = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n\n# Using an `Encoder`, we may add the audio data directly to a MongoDB collection:\nenc = array('float64', shape=(None,))\n\ndb.add(enc)\n\ndb.execute(voice_collection.insert_many([\n    Document({'audio': enc(r['audio']['array'])}) for r in data\n]))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"install-pre-trained-model-librespeech-into-database",children:"Install Pre-Trained Model (LibreSpeech) into Database"}),"\n",(0,t.jsxs)(n.p,{children:["Apply a pretrained ",(0,t.jsx)(n.code,{children:"transformers"})," model to the data:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\nfrom superduperdb.ext.transformers import Pipeline\n\nmodel = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\nprocessor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n\nSAMPLING_RATE = 16000\n\ntranscriber = Pipeline(\n    identifier='transcription',\n    object=model,\n    preprocess=processor,\n    preprocess_kwargs={'sampling_rate': SAMPLING_RATE, 'return_tensors': 'pt', 'padding': True},\n    postprocess=lambda x: processor.batch_decode(x, skip_special_tokens=True),\n    predict_method='generate',\n    preprocess_type='other',\n)\n"})}),"\n",(0,t.jsx)(n.h1,{id:"run-predictions-on-all-recordings-in-the-collection",children:"Run Predictions on All Recordings in the Collection"}),"\n",(0,t.jsxs)(n.p,{children:["Apply the ",(0,t.jsx)(n.code,{children:"Pipeline"})," to all audio recordings:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"transcriber.predict(X='audio', db=db, select=voice_collection.find(), max_chunk_size=10)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"ask-questions-to-your-voice-assistant",children:"Ask Questions to Your Voice Assistant"}),"\n",(0,t.jsx)(n.p,{children:"Ask questions to your voice assistant, targeting specific queries and utilizing the power of MongoDB for vector-search and filtering rules:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import VectorIndex, Listener\nfrom superduperdb.ext.openai import OpenAIEmbedding\n\ndb.add(\n    VectorIndex(\n        identifier='my-index',\n        indexing_listener=Listener(\n            model=OpenAIEmbedding(model='text-embedding-ada-002'),\n            key='_outputs.audio.transcription',\n            select=voice_collection.find(),\n        ),\n    )\n)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Let's confirm this has worked, by searching for the ",(0,t.jsx)(n.code,{children:"royal cavern"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Define the search parameters\nsearch_term = 'royal cavern'\nnum_results = 2\n\nlist(db.execute(\n    voice_collection.like(\n        {'_outputs.audio.transcription': search_term},\n        n=num_results,\n        vector_index='my-index',\n    ).find({}, {'_outputs.audio.transcription': 1})\n))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"enrich-it-with-chat-completion",children:"Enrich it with Chat-Completion"}),"\n",(0,t.jsxs)(n.p,{children:["Connect the previous steps with the gpt-3.5.turbo, a chat-completion model on OpenAI. The plan is to seed the completions with the most relevant audio recordings, as judged by their textual transcriptions. These transcriptions are retrieved using the previously configured ",(0,t.jsx)(n.code,{children:"VectorIndex"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb.ext.openai import OpenAIChatCompletion\n\nchat = OpenAIChatCompletion(\n    model='gpt-3.5-turbo',\n    prompt=(\n        'Use the following facts to answer this question\\n'\n        '{context}\\n\\n'\n        'Here\\'s the question:\\n'\n    ),\n)\n\ndb.add(chat)\n\nprint(db.show('model'))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"full-voice-assistant-experience",children:"Full Voice-Assistant Experience"}),"\n",(0,t.jsx)(n.p,{children:"Test the full model by asking a question about a specific fact mentioned in the audio recordings. The model will retrieve the most relevant recordings and use them to formulate its answer:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import Document\n\nq = 'Is anything really Greek?'\n\nprint(db.predict(\n    model_name='gpt-3.5-turbo',\n    input=q,\n    context_select=voice_collection.like(\n        Document({'_outputs.audio.transcription': q}), vector_index='my-index'\n    ).find(),\n    context_key='_outputs.audio.transcription',\n)[0].content)\n"})})]})}function p(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},11151:(e,n,o)=>{o.d(n,{Z:()=>a,a:()=>r});var t=o(67294);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);